---
title: 'Machine learning - Exercises'
author: 'Mun-Gwan Hong and Payam Emami'
date: today
format:
  html:
    code-fold: true
---

## Random forest

In this exercises, we will re-analyze a data set from `mixOmics` package.
According to the authors: 
“The Small Round Blue Cell Tumors (SRBCT) dataset from includes the expression levels of 2,308 genes measured on 63 samples. The samples are classified into four classes as follows: 8 Burkitt Lymphoma (BL), 23 Ewing Sarcoma (EWS), 12 neuroblastoma (NB), and 20 rhabdomyosarcoma (RMS).”

The dataset can be downloaded from [this link](https://github.com/mixOmicsTeam/mixOmics/raw/master/data/srbct.rda) and loaded into R by the following commands.

```{r}
#| code-fold: false
# Because direct download doesn't work from GitHub, we download the data to a temporary file.
tmp_file <- tempfile(fileext = "rda")
download.file("https://github.com/mixOmicsTeam/mixOmics/raw/master/data/srbct.rda", tmp_file)
load(tmp_file)    # loads `srbct`
```

After loading the data, we will have a variable called `srbct` which is a list containing the following:

* `gene`: a data frame with 63 rows and 2308 columns. The expression levels of 2,308 genes in 63 subjects.
* `class`: a class vector containing the class tumor of each individual (4 classes in total).
* `gene.name`: a data frame with 2,308 rows and 2 columns containing further information on the gene

We can combine `gene` and `class` into a single data frame using:

```{r}
#| code-fold: false
srbct_df <- data.frame(class = srbct$class, srbct$gene)
# optional, if you prefer `tibble` in `tidyverse` package set to `data.frame`
library(tidyverse)
srbct_df <- as_tibble(srbct_df)
```

We are going to use random forests to find variables that are important for discriminating the 4 classes.

1. Load the `randomForest` R package.

```{r}
library(randomForest)
```

2. Randomly split your data into a training (80% of the data) and testing sets (20% of the data).

```{r}
n_sample <- nrow(srbct_df)  # number of samples
idx_rnd <- sample(n_sample)    # randomly sampled indices
cutoff_0.8 <- floor(n_sample * 0.8)
idx_training <- idx_rnd[1:cutoff_0.8]
idx_test <- idx_rnd[(cutoff_0.8 + 1):n_sample]
df_training <- srbct_df[idx_training, ]
df_test <- srbct_df[idx_test, ]
```

3. Tune a hyper-parameter (`mtry`) of random forest (only on training data)

```{r}
y_train <- df_training[[1]]
x_train <- df_training[, -1]
rf_tune_res <- tuneRF(x = x_train, y = y_train, mtryStart = 100, ntreeTry = 100)
print(rf_tune_res)
```

4. Fit a random forest model to the training data and find its error rate (either out of bag error or cross validation. up to you!)

```{r}
rf <- randomForest(x = x_train, y = y_train, mtry = 200)
print(rf)
```

5. What is the accuracy of predicting the test set?

```{r}
y_test <- df_test[[1]]
x_test <- df_test[, -1]
y_pred <- predict(rf, newdata = x_test)
yardstick::accuracy_vec(y_test, y_pred)
```

6. Find the top 10 most important genes for discriminating all the classes

```{r}
rf <- randomForest(x = x_train, y = y_train, mtry = 200, importance = TRUE)
print(rf)
randomForest::importance(rf, type = 1)


```

7. What are the top 10 genes for predicting _EWS_ class

```{r}
randomForest::importance(rf) |> 
  print(0)
```

